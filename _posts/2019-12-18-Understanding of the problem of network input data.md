---
layout: post
title: 对含有全连接层的网络输入数据大小固定问题的理解
subtitle: finetune是一门学问
tags: [Deep Learning]
---

<!-- ## 对含有全连接层的网络输入数据大小固定问题的理解 -->

固定大小是说送入网络训练的每一个数据大小需要一样， 除了何凯明团队提出的 Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 

<https://arxiv.org/abs/1406.4729>

利用空间金字塔池化将任意大小的图像输出成固定的输出，然后输出到后面的全连接层解决了固定大小的问题。但是固定大小不代表网络输入必须是这个大小，比如说现在输入图像大小为227x227，那么你所有图像都必须为227x227, 不能出现不一样的, 但是你也可以将图像都改为224x224(没有预训练模型的情况下)。
　　解释：含有全连接层的网络输入数据的大小应该是固定的，这是因为全连接层和前面一层的连接的参数数量需要事先确定，不像卷积核的参数个数就是卷积核大小，前层的图像大小不管怎么变化，卷积核的参数数量也不会改变，但全连接的参数是随前层大小的变化而变的，如果输入图片大小不一样，那么全连接层之前的feature map也不一样，那全连接层的参数数量就不能确定， 所以必须实现固定输入图像的大小。
　　另外全连接的实现可以通过卷积的方式进行， 这里有两种情况：

均使用全卷积结构：

前层为卷积层或池化层（就是全连接层的输入是feature_map，而不是单个值）：
比如前层输出为512x5x5的feature map，全连接层有500个神经元，那这可以看做是用5x5的卷积核对前层的这些feature map进行卷积，最后输出500x1x1的feature map；

前层是全连接层
若前层有500个神经元，当前层有100个，那可以看做前层有500x1x1个feature map，然后用1x1的卷积核对这些feature map进行卷积，则得到100x1x1个feature map。
————————————————

众所周知CNN网络中常见结构是：卷积、池化和激活。卷积层是CNN网络的核心，激活函数帮助网络获得非线性特征，而池化的作用则体现在降采样：保留显著特征、降低特征维度，增大kernel的感受野。深度网络越往后面越能捕捉到物体的语义信息，这种语义信息是建立在较大的感受野基础上。已古人的例子来做解释，想必大家都知道盲人摸象这个成语的来历，每个盲人只能触摸到大象的一部分，也就是只能获得local response，基于这些local response，盲人们很难猜对他们到底在摸什么。即使是一个明眼人，眼睛紧贴这大象后背看，也很难猜到看的是什么。这个例子告诉我们局部信息很难提供更高层的语义信息，因此对feature map降维，进而增大后面各层kernel的感受野是一件很重要的事情。另外一点值得注意：pooling也可以提供一些旋转不变性。


当然，除了卷积层外，还可以用别的层替换全连接层。比如用**全局平均池化层**（Global Average Pooling）替换全连接层。可以探索到GAP的真正意义是:**对整个网路在结构上做正则化防止过拟合**

由此就可以比较直观地说明了。这两者合二为一的过程我们可以探索到GAP的真正意义是:**对整个网路在结构上做正则化防止过拟合**。其直接剔除了全连接层中黑箱的特征，直接赋予了每个channel实际的内别意义。
实践证明其效果还是比较可观的，同时GAP可以实现任意图像大小的输入。但是值得我们注意的是，使用gap可能会造成收敛速度减慢。

global average pooling 与 average pooling 的差别就在 "global" 这一个字眼上。global 与 local 在字面上都是用来形容 pooling 窗口区域的。 local 是取 feature map 的一个子区域求平均值，然后滑动这个子区域； global 显然就是对整个 feature map 求平均值了

传统的CNN架构，图像（image）经过裁剪或拉伸（crop/warp）统一尺寸，再传给卷积层（conv layers）；而应用了SPP层后的架构，图像直接传给卷积层，然后经过SPP处理，统一维度，再传给全连接层（fc layers）。

一些有的没的想法：

> 1.全连接层结构的模型，对于训练学习的过程，可能压力更多的在全连接层。就是说，卷积的特征学习的低级一些，没有关系，全连接不断学习调整参数，一样能很好的分类。
>
> 2.全局平均池化层代替全连接层的模型，学习训练的压力全部前导到卷积层。卷积的特征学习相较来说要更为"高级"一些。（因此收敛速度变慢?）
> 为什么这么想呢？我的理解是，全局平均池化较全连接层，应该会淡化不同特征间的相对位置的组合关系（“全局”的概念即如此）。因此，卷积训练出来的特征应该更加“高级”
>
> 3.以上的两个观点联合起来，可以推导出，全局平均池化层代替全连接层虽然有好处，但是不利于迁移学习。因为参数较为“固化”在卷积的诸层网络中。增加新的分类，那就意味着相当数量的卷积特征要做调整。而全连接层模型则可以更好的迁移学习，因为它的参数很大一部分调整在全连接层，迁移的时候卷积层可能也会调整，但是相对来讲要小的多了